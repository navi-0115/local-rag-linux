services:
  backend:
    build:
      context: .
      target: backend
    ports:
      - "8000:8000"
    environment:
      # Connect to existing Ollama container via host network
      - LLM_API_URL=http://host.docker.internal:11434
    # restart: unless-stopped

  frontend:
    build:
      context: .
      target: frontend
    ports:
      - "8501:8501"
    depends_on:
      - backend
    environment:
      - API_URL=http://backend:8000
    # restart: unless-stopped

networks:
  default:
    driver: bridge
