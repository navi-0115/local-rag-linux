services:
  backend:
    build:
      context: .
      target: backend
    ports:
      - "8000:8000"
    environment:
      - LLM_API_URL=http://ollama:11434
    depends_on:
      - ollama

  frontend:
    build:
      context: .
      target: frontend
    ports:
      - "8501:8501"
    depends_on:
      - backend
    environment:
      - API_URL=http://backend:8000

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama # For persisting Ollama models and data
    #   - ./ollama_setup:/model_files # MOUNT THE SCRIPT DIRECTORY
    # entrypoint: ["/bin/sh", "/model_files/run_ollama.sh"]
    restart: unless-stopped

networks:
  default:
    driver: bridge

volumes:
  ollama_data:
# services:
#   backend:
#     build:
#       context: .
#       target: backend
#     ports:
#       - "8000:8000"
#     environment:
#       # Connect to existing Ollama container via host network
#       - LLM_API_URL=http://host.docker.internal:11434
#     # restart: unless-stopped

#   frontend:
#     build:
#       context: .
#       target: frontend
#     ports:
#       - "8501:8501"
#     depends_on:
#       - backend
#     environment:
#       - API_URL=http://backend:8000
#     # restart: unless-stopped

# networks:
#   default:
#     driver: bridge
